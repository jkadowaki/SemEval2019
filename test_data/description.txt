I used the cased, BERT-based model with a maximum sequence length of 128, training batch size of 32, learning rate of 2*10^5. No cleaning or preprocessing was done to the text. I took the majority vote between epochs 2,3,4 (where epoch 0 corresponds to the base model with no fine-tuning). Those epochs were selected from a series of tests performed on the annotated dataset which attempted to find the optimal epochs to ensemble predictions.